# Building ETL Pipeline Using Azure Databricks

In this project, we need to use a major Big Data system to perform a Data Engineering related task. The example systems could be: (AWS Athena, AWS Spark/EMR, AWS Sagemaker, Databricks, Snowflake). I plan to to use Azure Databricks, which is a Spark-based unified analytics platform running on Microsoft Azure.

## Plan
1. learning the architecture and components of Azure Databricks
2. setting up the Databricks environment
3. building an end-to-end ETL pipline
4. orchestrating the pipeline
5. using Databricks APIs and Delta Lake to build automated pipelines (optinal)

## week1 progress
* Apache Spark
1. An in-memory analytics engine built on cluster computing technology
2. Processing in Spark happens on RDDs, which is the fundamental data structure of Spark, stored in the memory of cluster
* Databrcicks
1. Managed and optimized platform for running Apache Spark
2. Provides whole bunch of tools out of the box
3. Provides an integrated workspace to write the code and do real time collaboration
4. Allows you to setup the infrastructure with few clicks and leave the rest for Databricks to manage
* Getting start with Azure Databricks

After building a Azure Databricks Service, click `Launch Workspace`, the you would get the workspace. 
First, you need to create a cluster.
![cluster](https://github.com/JuliaJHL/imgs_readme/blob/main/ids721proj3/cluster.png)
Then, you could open a notebook to edit your work.
![notebook](https://github.com/JuliaJHL/imgs_readme/blob/main/ids721proj3/notebook.png)
You could also connect to GitHub via the settings.
![github](https://github.com/JuliaJHL/imgs_readme/blob/main/ids721proj3/github.png)





